#!/usr/bin/env python3
"""
SOC Operational Readiness Assessment
===================================

Comprehensive operational readiness testing for SOC deployment.
Validates SCAFAD's operational capabilities, monitoring integration,
alerting systems, and day-to-day SOC workflow compatibility.

Assessment Areas:
- Alert Generation and Management
- Integration with SIEM Systems
- Incident Response Workflows
- Monitoring and Observability
- Configuration Management
- Backup and Recovery
- Documentation and Runbooks
- Team Training Requirements
- Maintenance Procedures
- Compliance Reporting

Operational Standards:
- ITIL Service Management
- ISO 20000 IT Service Management
- NIST SP 800-61 Incident Handling
- SOC 2 Operational Controls
"""

import asyncio
import time
import json
import uuid
import tempfile
import os
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path

# SCAFAD imports
from app_main import Layer0_AdaptiveTelemetryController
from app_config import ScafadConfig
from app_telemetry import TelemetryRecord, AnomalyType, ExecutionPhase, TelemetrySource
from utils.metrics import PerformanceMetricsCollector


class OperationalArea(Enum):
    """Operational assessment areas"""
    ALERTING = "ALERTING"
    MONITORING = "MONITORING" 
    INCIDENT_RESPONSE = "INCIDENT_RESPONSE"
    CONFIGURATION = "CONFIGURATION"
    BACKUP_RECOVERY = "BACKUP_RECOVERY"
    DOCUMENTATION = "DOCUMENTATION"
    INTEGRATION = "INTEGRATION"
    MAINTENANCE = "MAINTENANCE"
    COMPLIANCE = "COMPLIANCE"
    TRAINING = "TRAINING"


class ReadinessLevel(Enum):
    """Operational readiness levels"""
    PRODUCTION_READY = "PRODUCTION_READY"
    CONDITIONALLY_READY = "CONDITIONALLY_READY"
    REQUIRES_PREPARATION = "REQUIRES_PREPARATION"
    NOT_READY = "NOT_READY"


@dataclass
class OperationalTestResult:
    """Individual operational test result"""
    test_id: str
    test_name: str
    area: OperationalArea
    passed: bool
    execution_time: float
    readiness_score: float  # 0-100
    findings: List[str] = field(default_factory=list)
    recommendations: List[str] = field(default_factory=list)
    details: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'test_id': self.test_id,
            'test_name': self.test_name,
            'area': self.area.value,
            'passed': self.passed,
            'execution_time': self.execution_time,
            'readiness_score': self.readiness_score,
            'findings': self.findings,
            'recommendations': self.recommendations,
            'details': self.details
        }


class SOCOperationalReadinessTestSuite:
    """SOC Operational Readiness Assessment Suite"""
    
    def __init__(self):
        self.config = ScafadConfig()
        self.controller = Layer0_AdaptiveTelemetryController(self.config)
        self.metrics_collector = PerformanceMetricsCollector()
        
        # Create temporary directories for testing
        self.test_dir = Path(tempfile.mkdtemp(prefix="scafad_ops_test_"))
        self.log_dir = self.test_dir / "logs"
        self.config_dir = self.test_dir / "config"
        self.backup_dir = self.test_dir / "backup"
        
        for dir_path in [self.log_dir, self.config_dir, self.backup_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
    
    async def test_alert_generation_system(self) -> OperationalTestResult:
        """Test alert generation and management capabilities"""
        test_start = time.time()
        test_id = "OPS_ALERT_001"
        
        findings = []
        recommendations = []
        alert_tests_passed = 0
        total_alert_tests = 0
        
        try:
            # Test different alert scenarios
            alert_scenarios = [
                {
                    "scenario": "critical_security_event",
                    "event": {
                        "event_type": "security_alert",
                        "severity": "CRITICAL",
                        "attack_type": "sql_injection",
                        "source_ip": "203.0.113.42",
                        "correlation_id": f"alert_test_critical_{uuid.uuid4()}"
                    },
                    "expected_alert": True,
                    "expected_priority": "HIGH"
                },
                {
                    "scenario": "anomaly_detection",
                    "event": {
                        "event_type": "anomaly",
                        "anomaly_score": 0.95,
                        "resource_usage": "excessive_cpu",
                        "correlation_id": f"alert_test_anomaly_{uuid.uuid4()}"
                    },
                    "expected_alert": True,
                    "expected_priority": "MEDIUM"
                },
                {
                    "scenario": "benign_event",
                    "event": {
                        "event_type": "normal_operation",
                        "status": "success",
                        "correlation_id": f"alert_test_benign_{uuid.uuid4()}"
                    },
                    "expected_alert": False,
                    "expected_priority": None
                },
                {
                    "scenario": "resource_exhaustion",
                    "event": {
                        "event_type": "resource_alert",
                        "memory_usage_pct": 95,
                        "cpu_usage_pct": 90,
                        "correlation_id": f"alert_test_resource_{uuid.uuid4()}"
                    },
                    "expected_alert": True,
                    "expected_priority": "HIGH"
                }
            ]
            
            context = type('Context', (), {
                'aws_request_id': 'ops_alert_test',
                'function_name': 'scafad_alert_test'
            })()
            
            for scenario in alert_scenarios:
                total_alert_tests += 1
                scenario_name = scenario["scenario"]
                
                # Process event
                result = await self.controller.process_telemetry_event(scenario["event"], context)
                
                # Check alert generation
                telemetry_record = result.get('telemetry_record', {})
                anomaly_type = telemetry_record.get('anomaly_type')
                
                # Check if alert should have been generated
                should_alert = scenario["expected_alert"]
                alert_generated = anomaly_type != 'BENIGN' if anomaly_type else False
                
                if should_alert == alert_generated:
                    alert_tests_passed += 1
                    findings.append(f"âœ“ {scenario_name}: Alert generation correct")
                else:
                    expected_str = "should generate" if should_alert else "should not generate"
                    actual_str = "generated" if alert_generated else "not generated"
                    findings.append(f"âœ— {scenario_name}: {expected_str} alert, but {actual_str}")
                
                # Check alert metadata
                if alert_generated and 'alert_metadata' in result:
                    alert_meta = result['alert_metadata']
                    if 'priority' in alert_meta and 'timestamp' in alert_meta:
                        findings.append(f"âœ“ {scenario_name}: Alert metadata complete")
                    else:
                        findings.append(f"âœ— {scenario_name}: Alert metadata incomplete")
                        recommendations.append("Include priority and timestamp in alert metadata")
            
            # Test alert deduplication
            duplicate_event = alert_scenarios[0]["event"].copy()
            result1 = await self.controller.process_telemetry_event(duplicate_event, context)
            result2 = await self.controller.process_telemetry_event(duplicate_event, context)
            
            # Check if deduplication works (simplified check)
            if 'alert_metadata' in result1 and 'alert_metadata' in result2:
                if result1.get('alert_metadata', {}).get('alert_id') == result2.get('alert_metadata', {}).get('alert_id'):
                    findings.append("âœ“ Alert deduplication working")
                else:
                    findings.append("âœ— Alert deduplication not working")
                    recommendations.append("Implement alert deduplication based on correlation ID")
            
            # Calculate readiness score
            alert_accuracy = alert_tests_passed / total_alert_tests if total_alert_tests > 0 else 0
            has_metadata = any("metadata complete" in f for f in findings)
            has_deduplication = any("deduplication working" in f for f in findings)
            
            readiness_score = (
                alert_accuracy * 60 +  # 60 points for accuracy
                (30 if has_metadata else 0) +  # 30 points for metadata
                (10 if has_deduplication else 0)  # 10 points for deduplication
            )
            
            test_passed = readiness_score >= 80
            
            if not test_passed:
                if alert_accuracy < 0.8:
                    recommendations.append("Improve alert generation accuracy")
                if not has_metadata:
                    recommendations.append("Implement comprehensive alert metadata")
                if not has_deduplication:
                    recommendations.append("Add alert deduplication capabilities")
            
            return OperationalTestResult(
                test_id=test_id,
                test_name="Alert Generation System",
                area=OperationalArea.ALERTING,
                passed=test_passed,
                execution_time=time.time() - test_start,
                readiness_score=readiness_score,
                findings=findings,
                recommendations=recommendations,
                details={
                    'alert_tests_passed': alert_tests_passed,
                    'total_alert_tests': total_alert_tests,
                    'alert_accuracy': alert_accuracy,
                    'scenarios_tested': len(alert_scenarios)
                }
            )
            
        except Exception as e:
            return OperationalTestResult(
                test_id=test_id,
                test_name="Alert Generation System",
                area=OperationalArea.ALERTING,
                passed=False,
                execution_time=time.time() - test_start,
                readiness_score=0.0,
                findings=[f"Test exception: {str(e)}"],
                recommendations=["Fix alert generation testing framework"],
                details={'error': str(e)}
            )
    
    async def test_monitoring_observability(self) -> OperationalTestResult:
        """Test monitoring and observability capabilities"""
        test_start = time.time()
        test_id = "OPS_MON_001"
        
        findings = []
        recommendations = []
        
        try:
            # Test metrics collection
            metrics_available = []
            
            # Check if performance metrics are being collected
            initial_report = self.metrics_collector.generate_performance_report()
            
            if 'system_metrics' in initial_report:
                metrics_available.append("system_metrics")
                findings.append("âœ“ System metrics collection available")
            else:
                findings.append("âœ— System metrics collection not available")
                recommendations.append("Implement system metrics collection")
            
            if 'metrics_summary' in initial_report:
                metrics_available.append("performance_metrics")
                findings.append("âœ“ Performance metrics collection available")
            else:
                findings.append("âœ— Performance metrics collection not available")
                recommendations.append("Implement performance metrics collection")
            
            # Test health check endpoint simulation
            context = type('Context', (), {
                'aws_request_id': 'health_check_test',
                'function_name': 'scafad_health_check'
            })()
            
            health_check_event = {
                "event_type": "health_check",
                "correlation_id": f"health_{uuid.uuid4()}"
            }
            
            try:
                health_result = await self.controller.process_telemetry_event(health_check_event, context)
                
                # Check health check response
                if health_result and isinstance(health_result, dict):
                    if 'status' in health_result:
                        findings.append("âœ“ Health check endpoint functional")
                    else:
                        findings.append("âœ— Health check response missing status")
                        recommendations.append("Include status field in health check response")
                    
                    # Check for component health details
                    if 'components' in health_result:
                        findings.append("âœ“ Component health reporting available")
                    else:
                        findings.append("âœ— Component health details not available")
                        recommendations.append("Add individual component health reporting")
                else:
                    findings.append("âœ— Health check endpoint not responding properly")
                    recommendations.append("Implement proper health check endpoint")
                    
            except Exception as e:
                findings.append(f"âœ— Health check failed: {str(e)}")
                recommendations.append("Fix health check implementation")
            
            # Test logging capabilities
            log_test_events = [
                {"level": "INFO", "message": "Test info message"},
                {"level": "WARN", "message": "Test warning message"},
                {"level": "ERROR", "message": "Test error message"},
            ]
            
            logging_working = True
            for log_event in log_test_events:
                test_event = {
                    "event_type": "log_test",
                    "log_level": log_event["level"],
                    "message": log_event["message"],
                    "correlation_id": f"log_test_{uuid.uuid4()}"
                }
                
                try:
                    await self.controller.process_telemetry_event(test_event, context)
                except Exception:
                    logging_working = False
                    break
            
            if logging_working:
                findings.append("âœ“ Logging system functional")
            else:
                findings.append("âœ— Logging system issues detected")
                recommendations.append("Fix logging system implementation")
            
            # Test telemetry export formats
            telemetry_formats = []
            
            # Check if telemetry can be exported in common formats
            sample_telemetry = TelemetryRecord(
                event_id="monitoring_test",
                timestamp=time.time(),
                function_id="test_function",
                execution_phase=ExecutionPhase.INVOKE,
                anomaly_type=AnomalyType.BENIGN,
                duration=1.0,
                memory_spike_kb=1024,
                cpu_utilization=50.0,
                network_io_bytes=1000,
                fallback_mode=False,
                source=TelemetrySource.PRIMARY,
                concurrency_id="mon_test"
            )
            
            try:
                # Test JSON export
                json_export = sample_telemetry.to_dict()
                if json_export and 'event_id' in json_export:
                    telemetry_formats.append("JSON")
                    findings.append("âœ“ JSON telemetry export available")
            except Exception:
                findings.append("âœ— JSON telemetry export failed")
                recommendations.append("Fix JSON telemetry export")
            
            # Calculate readiness score
            metrics_score = len(metrics_available) * 20  # 20 points per metric type
            health_check_score = 25 if any("Health check endpoint functional" in f for f in findings) else 0
            logging_score = 15 if logging_working else 0
            telemetry_score = len(telemetry_formats) * 10  # 10 points per format
            
            readiness_score = min(100, metrics_score + health_check_score + logging_score + telemetry_score)
            test_passed = readiness_score >= 70
            
            if not test_passed:
                recommendations.append("Enhance monitoring and observability capabilities")
                recommendations.append("Implement comprehensive health checks and metrics collection")
            
            return OperationalTestResult(
                test_id=test_id,
                test_name="Monitoring and Observability",
                area=OperationalArea.MONITORING,
                passed=test_passed,
                execution_time=time.time() - test_start,
                readiness_score=readiness_score,
                findings=findings,
                recommendations=recommendations,
                details={
                    'metrics_available': metrics_available,
                    'telemetry_formats': telemetry_formats,
                    'logging_functional': logging_working
                }
            )
            
        except Exception as e:
            return OperationalTestResult(
                test_id=test_id,
                test_name="Monitoring and Observability",
                area=OperationalArea.MONITORING,
                passed=False,
                execution_time=time.time() - test_start,
                readiness_score=0.0,
                findings=[f"Test exception: {str(e)}"],
                recommendations=["Fix monitoring and observability testing"],
                details={'error': str(e)}
            )
    
    async def test_configuration_management(self) -> OperationalTestResult:
        """Test configuration management capabilities"""
        test_start = time.time()
        test_id = "OPS_CFG_001"
        
        findings = []
        recommendations = []
        
        try:
            # Test configuration loading and validation
            config_tests_passed = 0
            total_config_tests = 0
            
            # Test 1: Default configuration
            total_config_tests += 1
            try:
                default_config = ScafadConfig()
                if default_config.verbosity and default_config.temporal_window_seconds > 0:
                    config_tests_passed += 1
                    findings.append("âœ“ Default configuration loads successfully")
                else:
                    findings.append("âœ— Default configuration incomplete")
                    recommendations.append("Ensure all required configuration parameters have defaults")
            except Exception as e:
                findings.append(f"âœ— Default configuration loading failed: {str(e)}")
                recommendations.append("Fix configuration loading mechanism")
            
            # Test 2: Configuration validation
            total_config_tests += 1
            try:
                # Test invalid configuration
                test_config = ScafadConfig()
                test_config.temporal_window_seconds = -1  # Invalid value
                
                # Check if validation catches this
                if hasattr(test_config, 'validate') and callable(getattr(test_config, 'validate')):
                    try:
                        test_config.validate()
                        findings.append("âœ— Configuration validation not catching invalid values")
                        recommendations.append("Implement strict configuration validation")
                    except Exception:
                        config_tests_passed += 1
                        findings.append("âœ“ Configuration validation working")
                else:
                    findings.append("âœ— Configuration validation method not available")
                    recommendations.append("Add configuration validation method")
                    
            except Exception as e:
                findings.append(f"âœ— Configuration validation test failed: {str(e)}")
            
            # Test 3: Configuration serialization/deserialization
            total_config_tests += 1
            try:
                original_config = ScafadConfig()
                
                # Test JSON serialization
                config_dict = {}
                for attr in dir(original_config):
                    if not attr.startswith('_') and not callable(getattr(original_config, attr)):
                        config_dict[attr] = getattr(original_config, attr)
                
                config_json = json.dumps(config_dict, default=str)
                parsed_config = json.loads(config_json)
                
                if parsed_config and 'verbosity' in parsed_config:
                    config_tests_passed += 1
                    findings.append("âœ“ Configuration serialization working")
                else:
                    findings.append("âœ— Configuration serialization failed")
                    recommendations.append("Implement configuration serialization support")
                    
            except Exception as e:
                findings.append(f"âœ— Configuration serialization test failed: {str(e)}")
                recommendations.append("Fix configuration serialization")
            
            # Test 4: Environment variable override
            total_config_tests += 1
            test_env_var = "SCAFAD_TEST_VAR"
            test_value = "test_override_value"
            
            # Simulate environment variable
            original_value = os.environ.get(test_env_var)
            os.environ[test_env_var] = test_value
            
            try:
                # Check if configuration can be overridden by environment
                # This is a simplified test - actual implementation may vary
                if os.environ.get(test_env_var) == test_value:
                    config_tests_passed += 1
                    findings.append("âœ“ Environment variable configuration override capable")
                else:
                    findings.append("âœ— Environment variable override not working")
                    recommendations.append("Implement environment variable configuration override")
            finally:
                # Restore original environment
                if original_value is not None:
                    os.environ[test_env_var] = original_value
                else:
                    os.environ.pop(test_env_var, None)
            
            # Test configuration backup/restore
            backup_test_passed = False
            try:
                # Create test configuration file
                test_config_file = self.config_dir / "test_config.json"
                test_config_data = {"test": "configuration", "version": "1.0"}
                
                with open(test_config_file, 'w') as f:
                    json.dump(test_config_data, f)
                
                # Test backup
                backup_file = self.backup_dir / "config_backup.json"
                with open(test_config_file, 'r') as src, open(backup_file, 'w') as dst:
                    dst.write(src.read())
                
                # Verify backup
                with open(backup_file, 'r') as f:
                    backup_data = json.load(f)
                
                if backup_data == test_config_data:
                    backup_test_passed = True
                    findings.append("âœ“ Configuration backup/restore capability")
                else:
                    findings.append("âœ— Configuration backup/restore failed")
                    recommendations.append("Implement configuration backup/restore procedures")
                    
            except Exception as e:
                findings.append(f"âœ— Configuration backup test failed: {str(e)}")
                recommendations.append("Implement configuration backup capabilities")
            
            # Calculate readiness score
            config_accuracy = config_tests_passed / total_config_tests if total_config_tests > 0 else 0
            backup_score = 20 if backup_test_passed else 0
            
            readiness_score = config_accuracy * 80 + backup_score  # 80 points for tests, 20 for backup
            test_passed = readiness_score >= 80
            
            return OperationalTestResult(
                test_id=test_id,
                test_name="Configuration Management",
                area=OperationalArea.CONFIGURATION,
                passed=test_passed,
                execution_time=time.time() - test_start,
                readiness_score=readiness_score,
                findings=findings,
                recommendations=recommendations,
                details={
                    'config_tests_passed': config_tests_passed,
                    'total_config_tests': total_config_tests,
                    'backup_test_passed': backup_test_passed,
                    'config_accuracy': config_accuracy
                }
            )
            
        except Exception as e:
            return OperationalTestResult(
                test_id=test_id,
                test_name="Configuration Management",
                area=OperationalArea.CONFIGURATION,
                passed=False,
                execution_time=time.time() - test_start,
                readiness_score=0.0,
                findings=[f"Test exception: {str(e)}"],
                recommendations=["Fix configuration management testing"],
                details={'error': str(e)}
            )
    
    async def test_incident_response_integration(self) -> OperationalTestResult:
        """Test incident response workflow integration"""
        test_start = time.time()
        test_id = "OPS_IR_001"
        
        findings = []
        recommendations = []
        
        try:
            # Test incident creation and workflow
            incident_tests = []
            
            # Test 1: Security incident detection and escalation
            security_incident = {
                "event_type": "security_incident",
                "incident_type": "data_breach_attempt",
                "severity": "HIGH",
                "source_ip": "203.0.113.42",
                "target_resource": "user_database",
                "attack_vector": "sql_injection",
                "correlation_id": f"incident_test_{uuid.uuid4()}"
            }
            
            context = type('Context', (), {
                'aws_request_id': 'incident_response_test',
                'function_name': 'scafad_ir_test'
            })()
            
            try:
                ir_result = await self.controller.process_telemetry_event(security_incident, context)
                
                # Check if incident response was triggered
                if 'incident_response' in ir_result:
                    incident_data = ir_result['incident_response']
                    
                    # Check required incident fields
                    required_fields = ['incident_id', 'severity', 'status', 'created_at']
                    missing_fields = [field for field in required_fields if field not in incident_data]
                    
                    if not missing_fields:
                        incident_tests.append({"test": "incident_creation", "passed": True})
                        findings.append("âœ“ Security incident creation with required fields")
                    else:
                        incident_tests.append({"test": "incident_creation", "passed": False})
                        findings.append(f"âœ— Security incident missing fields: {missing_fields}")
                        recommendations.append("Include all required fields in incident creation")
                    
                    # Check incident classification
                    if 'classification' in incident_data:
                        incident_tests.append({"test": "incident_classification", "passed": True})
                        findings.append("âœ“ Incident classification available")
                    else:
                        incident_tests.append({"test": "incident_classification", "passed": False})
                        findings.append("âœ— Incident classification not available")
                        recommendations.append("Implement incident classification system")
                    
                    # Check timeline tracking
                    if 'timeline' in incident_data:
                        incident_tests.append({"test": "timeline_tracking", "passed": True})
                        findings.append("âœ“ Incident timeline tracking available")
                    else:
                        incident_tests.append({"test": "timeline_tracking", "passed": False})
                        findings.append("âœ— Incident timeline tracking not available")
                        recommendations.append("Implement incident timeline tracking")
                        
                else:
                    incident_tests.append({"test": "incident_creation", "passed": False})
                    findings.append("âœ— Security incident response not triggered")
                    recommendations.append("Implement incident response workflow")
                    
            except Exception as e:
                incident_tests.append({"test": "incident_creation", "passed": False})
                findings.append(f"âœ— Security incident processing failed: {str(e)}")
                recommendations.append("Fix incident response processing")
            
            # Test 2: Evidence collection
            evidence_event = {
                "event_type": "evidence_collection",
                "incident_id": "test_incident_001",
                "evidence_type": "network_logs",
                "source": "firewall",
                "correlation_id": f"evidence_test_{uuid.uuid4()}"
            }
            
            try:
                evidence_result = await self.controller.process_telemetry_event(evidence_event, context)
                
                if 'evidence_collection' in evidence_result:
                    incident_tests.append({"test": "evidence_collection", "passed": True})
                    findings.append("âœ“ Evidence collection capability available")
                else:
                    incident_tests.append({"test": "evidence_collection", "passed": False})
                    findings.append("âœ— Evidence collection capability not available")
                    recommendations.append("Implement evidence collection workflows")
                    
            except Exception as e:
                incident_tests.append({"test": "evidence_collection", "passed": False})
                findings.append(f"âœ— Evidence collection test failed: {str(e)}")
                recommendations.append("Fix evidence collection implementation")
            
            # Test 3: Notification system
            notification_test_passed = False
            try:
                # Simulate notification generation
                high_severity_event = {
                    "event_type": "critical_alert",
                    "severity": "CRITICAL",
                    "message": "Critical security event detected",
                    "correlation_id": f"notification_test_{uuid.uuid4()}"
                }
                
                notification_result = await self.controller.process_telemetry_event(high_severity_event, context)
                
                # Check if notification metadata is present
                if any('notification' in str(notification_result).lower() for _ in [1]):
                    notification_test_passed = True
                    incident_tests.append({"test": "notification_system", "passed": True})
                    findings.append("âœ“ Notification system integration available")
                else:
                    incident_tests.append({"test": "notification_system", "passed": False})
                    findings.append("âœ— Notification system integration not available")
                    recommendations.append("Implement notification system integration")
                    
            except Exception as e:
                incident_tests.append({"test": "notification_system", "passed": False})
                findings.append(f"âœ— Notification system test failed: {str(e)}")
                recommendations.append("Fix notification system implementation")
            
            # Test 4: Escalation procedures
            escalation_test = {
                "event_type": "incident_escalation",
                "incident_id": "test_incident_001",
                "escalation_reason": "severity_increase",
                "new_severity": "CRITICAL",
                "correlation_id": f"escalation_test_{uuid.uuid4()}"
            }
            
            try:
                escalation_result = await self.controller.process_telemetry_event(escalation_test, context)
                
                # Check escalation handling
                if 'escalation' in str(escalation_result).lower():
                    incident_tests.append({"test": "escalation_procedures", "passed": True})
                    findings.append("âœ“ Incident escalation procedures available")
                else:
                    incident_tests.append({"test": "escalation_procedures", "passed": False})
                    findings.append("âœ— Incident escalation procedures not available")
                    recommendations.append("Implement incident escalation procedures")
                    
            except Exception as e:
                incident_tests.append({"test": "escalation_procedures", "passed": False})
                findings.append(f"âœ— Escalation procedures test failed: {str(e)}")
                recommendations.append("Fix escalation procedures implementation")
            
            # Calculate readiness score
            passed_tests = sum(1 for test in incident_tests if test["passed"])
            total_tests = len(incident_tests)
            
            readiness_score = (passed_tests / total_tests * 100) if total_tests > 0 else 0
            test_passed = readiness_score >= 75  # 75% threshold for incident response
            
            if not test_passed:
                recommendations.append("Enhance incident response workflow integration")
                recommendations.append("Implement comprehensive incident management capabilities")
            
            return OperationalTestResult(
                test_id=test_id,
                test_name="Incident Response Integration",
                area=OperationalArea.INCIDENT_RESPONSE,
                passed=test_passed,
                execution_time=time.time() - test_start,
                readiness_score=readiness_score,
                findings=findings,
                recommendations=recommendations,
                details={
                    'incident_tests': incident_tests,
                    'passed_tests': passed_tests,
                    'total_tests': total_tests
                }
            )
            
        except Exception as e:
            return OperationalTestResult(
                test_id=test_id,
                test_name="Incident Response Integration",
                area=OperationalArea.INCIDENT_RESPONSE,
                passed=False,
                execution_time=time.time() - test_start,
                readiness_score=0.0,
                findings=[f"Test exception: {str(e)}"],
                recommendations=["Fix incident response testing framework"],
                details={'error': str(e)}
            )
    
    async def test_compliance_reporting(self) -> OperationalTestResult:
        """Test compliance reporting capabilities"""
        test_start = time.time()
        test_id = "OPS_COMP_001"
        
        findings = []
        recommendations = []
        
        try:
            # Test audit log generation
            audit_tests = []
            
            context = type('Context', (), {
                'aws_request_id': 'compliance_test',
                'function_name': 'scafad_compliance_test'
            })()
            
            # Test 1: Audit trail generation
            audit_event = {
                "event_type": "audit_test",
                "action": "security_analysis",
                "user": "system",
                "resource": "telemetry_processor",
                "correlation_id": f"audit_test_{uuid.uuid4()}"
            }
            
            try:
                audit_result = await self.controller.process_telemetry_event(audit_event, context)
                
                # Check if audit information is captured
                telemetry_record = audit_result.get('telemetry_record', {})
                
                audit_fields = ['event_id', 'timestamp', 'function_id']
                missing_audit_fields = [field for field in audit_fields if not telemetry_record.get(field)]
                
                if not missing_audit_fields:
                    audit_tests.append({"test": "audit_trail", "passed": True})
                    findings.append("âœ“ Audit trail generation working")
                else:
                    audit_tests.append({"test": "audit_trail", "passed": False})
                    findings.append(f"âœ— Audit trail missing fields: {missing_audit_fields}")
                    recommendations.append("Ensure all audit trail fields are captured")
                    
            except Exception as e:
                audit_tests.append({"test": "audit_trail", "passed": False})
                findings.append(f"âœ— Audit trail test failed: {str(e)}")
                recommendations.append("Fix audit trail generation")
            
            # Test 2: Compliance report generation
            try:
                # Generate a sample compliance report
                compliance_data = {
                    "period": "2024-01",
                    "events_processed": 1000,
                    "security_events": 50,
                    "incidents_created": 5,
                    "false_positives": 2,
                    "response_times": [1.2, 0.8, 2.1, 1.5, 0.9]
                }
                
                # Test report structure
                required_report_fields = ['period', 'events_processed', 'security_events']
                has_required_fields = all(field in compliance_data for field in required_report_fields)
                
                if has_required_fields:
                    audit_tests.append({"test": "compliance_report", "passed": True})
                    findings.append("âœ“ Compliance report structure available")
                else:
                    audit_tests.append({"test": "compliance_report", "passed": False})
                    findings.append("âœ— Compliance report structure incomplete")
                    recommendations.append("Implement comprehensive compliance reporting")
                    
            except Exception as e:
                audit_tests.append({"test": "compliance_report", "passed": False})
                findings.append(f"âœ— Compliance report test failed: {str(e)}")
                recommendations.append("Fix compliance report generation")
            
            # Test 3: Data retention policy compliance
            try:
                # Test data retention simulation
                old_event = {
                    "event_type": "retention_test",
                    "timestamp": time.time() - (365 * 24 * 3600),  # 1 year old
                    "correlation_id": f"retention_test_{uuid.uuid4()}"
                }
                
                retention_result = await self.controller.process_telemetry_event(old_event, context)
                
                # Check if retention policy is considered
                if 'retention_policy' in retention_result or 'data_lifecycle' in retention_result:
                    audit_tests.append({"test": "data_retention", "passed": True})
                    findings.append("âœ“ Data retention policy compliance available")
                else:
                    audit_tests.append({"test": "data_retention", "passed": False})
                    findings.append("âœ— Data retention policy compliance not available")
                    recommendations.append("Implement data retention policy compliance")
                    
            except Exception as e:
                audit_tests.append({"test": "data_retention", "passed": False})
                findings.append(f"âœ— Data retention test failed: {str(e)}")
                recommendations.append("Fix data retention policy implementation")
            
            # Test 4: Regulatory compliance checks
            regulatory_tests = [
                {"regulation": "GDPR", "requirement": "data_processing_consent"},
                {"regulation": "SOX", "requirement": "financial_data_protection"},
                {"regulation": "HIPAA", "requirement": "healthcare_data_security"},
                {"regulation": "PCI_DSS", "requirement": "payment_data_protection"}
            ]
            
            compliance_coverage = 0
            for reg_test in regulatory_tests:
                regulation = reg_test["regulation"]
                requirement = reg_test["requirement"]
                
                # Simulate regulatory compliance check
                regulatory_event = {
                    "event_type": "compliance_check",
                    "regulation": regulation,
                    "requirement": requirement,
                    "correlation_id": f"regulatory_test_{uuid.uuid4()}"
                }
                
                try:
                    reg_result = await self.controller.process_telemetry_event(regulatory_event, context)
                    
                    # Check if regulatory compliance is addressed
                    if regulation.lower() in str(reg_result).lower():
                        compliance_coverage += 1
                        findings.append(f"âœ“ {regulation} compliance consideration available")
                    else:
                        findings.append(f"âœ— {regulation} compliance consideration not available")
                        
                except Exception:
                    findings.append(f"âœ— {regulation} compliance test failed")
            
            if compliance_coverage >= len(regulatory_tests) // 2:  # At least half covered
                audit_tests.append({"test": "regulatory_compliance", "passed": True})
            else:
                audit_tests.append({"test": "regulatory_compliance", "passed": False})
                recommendations.append("Enhance regulatory compliance coverage")
            
            # Calculate readiness score
            passed_tests = sum(1 for test in audit_tests if test["passed"])
            total_tests = len(audit_tests)
            compliance_score = compliance_coverage / len(regulatory_tests) * 25  # 25 points for regulatory
            
            base_score = (passed_tests / total_tests * 75) if total_tests > 0 else 0
            readiness_score = base_score + compliance_score
            
            test_passed = readiness_score >= 70
            
            return OperationalTestResult(
                test_id=test_id,
                test_name="Compliance Reporting",
                area=OperationalArea.COMPLIANCE,
                passed=test_passed,
                execution_time=time.time() - test_start,
                readiness_score=readiness_score,
                findings=findings,
                recommendations=recommendations,
                details={
                    'audit_tests': audit_tests,
                    'passed_tests': passed_tests,
                    'total_tests': total_tests,
                    'compliance_coverage': compliance_coverage,
                    'regulatory_tests': len(regulatory_tests)
                }
            )
            
        except Exception as e:
            return OperationalTestResult(
                test_id=test_id,
                test_name="Compliance Reporting",
                area=OperationalArea.COMPLIANCE,
                passed=False,
                execution_time=time.time() - test_start,
                readiness_score=0.0,
                findings=[f"Test exception: {str(e)}"],
                recommendations=["Fix compliance reporting testing framework"],
                details={'error': str(e)}
            )
    
    async def run_comprehensive_operational_assessment(self) -> Dict[str, Any]:
        """Run comprehensive operational readiness assessment"""
        print("ðŸ¢ SOC Operational Readiness Assessment")
        print("=" * 50)
        
        assessment_start_time = time.time()
        
        # Define operational test suite
        operational_tests = [
            ("Alert Generation System", self.test_alert_generation_system),
            ("Monitoring and Observability", self.test_monitoring_observability),
            ("Configuration Management", self.test_configuration_management),
            ("Incident Response Integration", self.test_incident_response_integration),
            ("Compliance Reporting", self.test_compliance_reporting),
        ]
        
        all_results = []
        
        # Execute operational tests
        for test_name, test_method in operational_tests:
            print(f"\nðŸ”§ Testing {test_name}...")
            
            try:
                result = await test_method()
                all_results.append(result)
                
                status = "âœ… READY" if result.passed else "âš ï¸ NEEDS WORK"
                score_str = f"({result.readiness_score:.1f}/100)"
                
                print(f"   {status} {score_str} - {result.execution_time:.2f}s")
                
                if not result.passed and result.recommendations:
                    print(f"   Key recommendations: {len(result.recommendations)}")
                
            except Exception as e:
                print(f"   âŒ ERROR - {str(e)}")
                all_results.append(OperationalTestResult(
                    test_id=f"OPS_ERR_{len(all_results)}",
                    test_name=test_name,
                    area=OperationalArea.MAINTENANCE,
                    passed=False,
                    execution_time=0.0,
                    readiness_score=0.0,
                    findings=[f"Test execution failed: {str(e)}"],
                    recommendations=["Fix operational testing framework"]
                ))
        
        total_assessment_time = time.time() - assessment_start_time
        
        # Generate operational readiness report
        readiness_report = self.generate_operational_readiness_report(all_results, total_assessment_time)
        
        return readiness_report
    
    def generate_operational_readiness_report(self, results: List[OperationalTestResult], total_time: float) -> Dict[str, Any]:
        """Generate comprehensive operational readiness report"""
        
        total_tests = len(results)
        passed_tests = sum(1 for r in results if r.passed)
        failed_tests = total_tests - passed_tests
        
        # Calculate overall readiness score
        overall_readiness_score = sum(r.readiness_score for r in results) / total_tests if total_tests > 0 else 0
        
        # Group results by operational area
        by_area = {}
        for result in results:
            area = result.area.value
            if area not in by_area:
                by_area[area] = {'tests': [], 'avg_score': 0, 'passed': 0, 'total': 0}
            
            by_area[area]['tests'].append(result.to_dict())
            by_area[area]['total'] += 1
            if result.passed:
                by_area[area]['passed'] += 1
        
        # Calculate average scores by area
        for area_data in by_area.values():
            if area_data['tests']:
                area_data['avg_score'] = sum(t['readiness_score'] for t in area_data['tests']) / len(area_data['tests'])
        
        # Determine overall readiness level
        if overall_readiness_score >= 90 and passed_tests == total_tests:
            readiness_level = ReadinessLevel.PRODUCTION_READY
        elif overall_readiness_score >= 75 and passed_tests >= total_tests * 0.8:
            readiness_level = ReadinessLevel.CONDITIONALLY_READY
        elif overall_readiness_score >= 60:
            readiness_level = ReadinessLevel.REQUIRES_PREPARATION
        else:
            readiness_level = ReadinessLevel.NOT_READY
        
        # Collect all recommendations
        all_recommendations = []
        for result in results:
            all_recommendations.extend(result.recommendations)
        
        # Remove duplicates and prioritize
        unique_recommendations = list(dict.fromkeys(all_recommendations))
        
        # Generate deployment readiness assessment
        deployment_blockers = []
        critical_issues = []
        
        for result in results:
            if not result.passed:
                if result.readiness_score < 50:
                    deployment_blockers.append(f"{result.test_name}: Score {result.readiness_score:.1f}/100")
                else:
                    critical_issues.append(f"{result.test_name}: Score {result.readiness_score:.1f}/100")
        
        # SOC team requirements
        soc_team_requirements = self.generate_soc_team_requirements(results, readiness_level)
        
        # Generate operational runbook
        operational_runbook = self.generate_operational_runbook(results)
        
        report = {
            'report_metadata': {
                'generated_at': time.time(),
                'total_assessment_time': total_time,
                'scafad_version': '1.0.0',
                'assessment_type': 'operational_readiness'
            },
            'executive_summary': {
                'overall_readiness_level': readiness_level.value,
                'overall_readiness_score': overall_readiness_score,
                'production_ready': readiness_level == ReadinessLevel.PRODUCTION_READY,
                'tests_passed': passed_tests,
                'total_tests': total_tests,
                'deployment_blockers': len(deployment_blockers),
                'critical_issues': len(critical_issues)
            },
            'readiness_by_area': by_area,
            'detailed_results': [r.to_dict() for r in results],
            'deployment_assessment': {
                'readiness_level': readiness_level.value,
                'deployment_blockers': deployment_blockers,
                'critical_issues': critical_issues,
                'go_live_approved': len(deployment_blockers) == 0
            },
            'recommendations': {
                'immediate_actions': unique_recommendations[:5],
                'all_recommendations': unique_recommendations
            },
            'soc_team_requirements': soc_team_requirements,
            'operational_runbook': operational_runbook,
            'next_steps': self.generate_operational_next_steps(readiness_level, deployment_blockers)
        }
        
        return report
    
    def generate_soc_team_requirements(self, results: List[OperationalTestResult], readiness_level: ReadinessLevel) -> Dict[str, Any]:
        """Generate SOC team requirements and training needs"""
        
        training_requirements = []
        skill_requirements = []
        tooling_requirements = []
        
        # Analyze results for training needs
        failed_areas = [r.area.value for r in results if not r.passed]
        
        if OperationalArea.ALERTING.value in failed_areas:
            training_requirements.append("Alert triage and escalation procedures")
            skill_requirements.append("Alert analysis and correlation")
        
        if OperationalArea.INCIDENT_RESPONSE.value in failed_areas:
            training_requirements.append("Incident response workflow and SCAFAD integration")
            skill_requirements.append("Digital forensics and incident analysis")
        
        if OperationalArea.MONITORING.value in failed_areas:
            training_requirements.append("System monitoring and performance analysis")
            tooling_requirements.append("Monitoring dashboard configuration")
        
        # Base requirements for all SOC teams
        base_training = [
            "SCAFAD system overview and architecture",
            "Anomaly detection and false positive analysis",
            "Security event correlation and analysis",
            "Compliance reporting and audit procedures"
        ]
        
        base_skills = [
            "Security analysis and threat hunting",
            "Log analysis and pattern recognition",
            "Incident response and communication",
            "Regulatory compliance understanding"
        ]
        
        base_tooling = [
            "SCAFAD web interface and dashboards",
            "Alert management and ticketing system",
            "Compliance reporting tools",
            "Backup and configuration management tools"
        ]
        
        return {
            'training_requirements': base_training + training_requirements,
            'skill_requirements': base_skills + skill_requirements,
            'tooling_requirements': base_tooling + tooling_requirements,
            'team_size_recommendation': self._calculate_team_size_recommendation(readiness_level),
            'roles_needed': [
                "SOC Analyst I (Alert triage and initial analysis)",
                "SOC Analyst II (Advanced analysis and incident response)",
                "SOC Team Lead (Escalation and coordination)",
                "System Administrator (SCAFAD maintenance and configuration)"
            ]
        }
    
    def _calculate_team_size_recommendation(self, readiness_level: ReadinessLevel) -> Dict[str, int]:
        """Calculate recommended team size based on readiness level"""
        if readiness_level == ReadinessLevel.PRODUCTION_READY:
            return {
                "minimum_team_size": 6,
                "recommended_team_size": 8,
                "24x7_coverage": True,
                "shifts": 3
            }
        elif readiness_level == ReadinessLevel.CONDITIONALLY_READY:
            return {
                "minimum_team_size": 4,
                "recommended_team_size": 6,
                "24x7_coverage": False,
                "shifts": 2
            }
        else:
            return {
                "minimum_team_size": 2,
                "recommended_team_size": 4,
                "24x7_coverage": False,
                "shifts": 1
            }
    
    def generate_operational_runbook(self, results: List[OperationalTestResult]) -> Dict[str, List[str]]:
        """Generate operational runbook based on assessment results"""
        
        runbook = {
            'daily_operations': [
                "Monitor SCAFAD alert dashboard for new security events",
                "Review and triage alerts based on severity and priority",
                "Investigate anomalies and potential security incidents",
                "Update incident tickets and document findings",
                "Generate daily security summary reports"
            ],
            'weekly_operations': [
                "Review system performance metrics and trends",
                "Analyze false positive rates and tune detection rules",
                "Conduct configuration backup and validation",
                "Review compliance reporting and audit logs",
                "Update threat intelligence and detection signatures"
            ],
            'monthly_operations': [
                "Perform comprehensive system health assessment",
                "Review and update incident response procedures",
                "Conduct team training and knowledge sharing sessions",
                "Generate monthly compliance and performance reports",
                "Review and update operational documentation"
            ],
            'emergency_procedures': [
                "Critical alert escalation path and contact information",
                "System outage response and recovery procedures",
                "Security incident containment and isolation steps",
                "Emergency communication and notification protocols",
                "Disaster recovery and business continuity procedures"
            ],
            'maintenance_procedures': [
                "Regular system updates and patch management",
                "Configuration management and version control",
                "Database maintenance and optimization",
                "Log rotation and storage management",
                "Performance tuning and capacity planning"
            ]
        }
        
        # Customize based on assessment results
        failed_areas = [r.area.value for r in results if not r.passed]
        
        if OperationalArea.MONITORING.value in failed_areas:
            runbook['daily_operations'].insert(0, "Verify system monitoring and metrics collection")
        
        if OperationalArea.CONFIGURATION.value in failed_areas:
            runbook['weekly_operations'].insert(0, "Validate configuration integrity and settings")
        
        return runbook
    
    def generate_operational_next_steps(self, readiness_level: ReadinessLevel, deployment_blockers: List[str]) -> List[str]:
        """Generate next steps based on operational readiness assessment"""
        
        if readiness_level == ReadinessLevel.PRODUCTION_READY:
            return [
                "âœ… Approved for production SOC deployment",
                "Schedule go-live date and implementation timeline",
                "Conduct final SOC team training and readiness verification",
                "Implement production monitoring and alerting",
                "Plan post-deployment monitoring and optimization"
            ]
        elif readiness_level == ReadinessLevel.CONDITIONALLY_READY:
            return [
                "Address remaining operational gaps before full deployment",
                "Consider phased deployment with limited scope",
                "Implement additional monitoring and safeguards",
                "Schedule regular readiness assessments",
                "Plan operational team augmentation"
            ]
        elif readiness_level == ReadinessLevel.REQUIRES_PREPARATION:
            return [
                "âš ï¸ Significant preparation required before deployment",
                "Address all identified operational deficiencies",
                "Implement comprehensive training program",
                "Establish operational procedures and documentation",
                "Conduct follow-up readiness assessment in 30 days"
            ]
        else:  # NOT_READY
            return [
                "ðŸ›‘ NOT READY for production SOC deployment",
                "Address all deployment blockers immediately",
                f"Critical issues to resolve: {len(deployment_blockers)}",
                "Implement comprehensive operational improvements",
                "Conduct full operational readiness reassessment"
            ]
    
    def cleanup(self):
        """Cleanup test resources"""
        try:
            import shutil
            if self.test_dir.exists():
                shutil.rmtree(self.test_dir)
        except Exception as e:
            print(f"Warning: Could not cleanup test directory: {e}")


# Main execution
async def main():
    """Execute operational readiness assessment"""
    assessment_suite = SOCOperationalReadinessTestSuite()
    
    try:
        # Run comprehensive operational assessment
        readiness_report = await assessment_suite.run_comprehensive_operational_assessment()
        
        # Print executive summary
        print("\n" + "=" * 50)
        print("ðŸ¢ OPERATIONAL READINESS SUMMARY")
        print("=" * 50)
        
        summary = readiness_report['executive_summary']
        deployment = readiness_report['deployment_assessment']
        
        print(f"Readiness Level: {summary['overall_readiness_level']}")
        print(f"Readiness Score: {summary['overall_readiness_score']:.1f}/100")
        print(f"Production Ready: {'âœ… YES' if summary['production_ready'] else 'âŒ NO'}")
        print(f"Tests Passed: {summary['tests_passed']}/{summary['total_tests']}")
        print(f"Deployment Blockers: {summary['deployment_blockers']}")
        print(f"Critical Issues: {summary['critical_issues']}")
        
        # Deployment decision
        if deployment['go_live_approved']:
            print(f"\nðŸŽ‰ GO-LIVE APPROVED for SOC deployment")
        else:
            print(f"\nâš ï¸ GO-LIVE NOT APPROVED - Blockers must be resolved")
            if deployment['deployment_blockers']:
                print("   Deployment Blockers:")
                for blocker in deployment['deployment_blockers'][:3]:
                    print(f"   â€¢ {blocker}")
        
        # Key recommendations
        recommendations = readiness_report['recommendations']
        if recommendations['immediate_actions']:
            print(f"\nðŸ“‹ Immediate Actions Required:")
            for action in recommendations['immediate_actions']:
                print(f"   â€¢ {action}")
        
        # Save detailed report
        report_filename = f"scafad_operational_readiness_report_{int(time.time())}.json"
        with open(report_filename, 'w') as f:
            json.dump(readiness_report, f, indent=2, default=str)
        
        print(f"\nðŸ“„ Detailed report saved to: {report_filename}")
        
        return readiness_report
        
    finally:
        # Cleanup test resources
        assessment_suite.cleanup()


if __name__ == "__main__":
    report = asyncio.run(main())