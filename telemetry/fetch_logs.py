# telemetry/fetch_logs.py

import json
import csv
import subprocess
import os
import time
from datetime import datetime
from collections import Counter

# --- AWS Lambda Log Group ---
log_group = "/aws/lambda/scafad-test-stack-HelloWorldFunction-k79tX3iBcK74"  # ‚úÖ Change if needed

# --- Ensure Archive Folder Exists ---
os.makedirs("telemetry/archive", exist_ok=True)

# --- Fetch Logs from CloudWatch ---
print("üì° Fetching logs from CloudWatch...")
result = subprocess.run(
    [
        "aws", "logs", "filter-log-events",
        "--log-group-name", log_group,
        "--limit", "100",
        "--start-time", str(int((time.time() - 3600) * 1000))  # ‚úÖ Last 60 minutes
    ],
    capture_output=True,
    text=True
)

# --- Handle CLI Errors ---
if not result.stdout:
    print("‚ùå No output received. Check AWS credentials, region, or log group name.")
    print(f"üîé stderr: {result.stderr.strip()}")
    exit(1)

try:
    log_data = json.loads(result.stdout)
except json.JSONDecodeError:
    print("‚ùå Failed to parse AWS CLI output as JSON.")
    exit(1)

events = log_data.get("events", [])

# --- Log Containers ---
side_channel_logs = []
primary_logs = []
malformed_count = 0
fieldnames = set()

# --- Parse Logs ---
for event in events:
    message = event.get("message", "")
    if message.startswith("[SCAFAD_TRACE]"):
        side_channel_logs.append({"raw_trace": message})
    else:
        try:
            log_entry = json.loads(message)
            primary_logs.append(log_entry)
            fieldnames.update(log_entry.keys())
        except json.JSONDecodeError:
            malformed_count += 1

# --- Filenames for Archive ---
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
csv_filename = f"telemetry/archive/lambda_telemetry_{timestamp}.csv"
trace_filename = f"telemetry/archive/side_channel_trace_{timestamp}.log"

# --- Save Structured Logs to CSV ---
if primary_logs:
    with open("telemetry/lambda_telemetry.csv", "w", newline='', encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=sorted(fieldnames))
        writer.writeheader()
        writer.writerows(primary_logs)

    with open(csv_filename, "w", newline='', encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=sorted(fieldnames))
        writer.writeheader()
        writer.writerows(primary_logs)

    print(f"‚úÖ Extracted {len(primary_logs)} structured logs to:")
    print(f"   ‚Üí telemetry/lambda_telemetry.csv")
    print(f"   ‚Üí {csv_filename}")
else:
    print("‚ö†Ô∏è No structured logs found.")

# --- Save Side Channel Logs ---
if side_channel_logs:
    with open("telemetry/side_channel_trace.log", "w", encoding="utf-8") as f:
        for entry in side_channel_logs:
            f.write(entry["raw_trace"] + "\n")

    with open(trace_filename, "w", encoding="utf-8") as f:
        for entry in side_channel_logs:
            f.write(entry["raw_trace"] + "\n")

    print(f"‚úÖ Extracted {len(side_channel_logs)} side traces to:")
    print(f"   ‚Üí telemetry/side_channel_trace.log")
    print(f"   ‚Üí {trace_filename}")
else:
    print("‚ö†Ô∏è No side-channel traces found.")

# --- Summary Stats ---
if primary_logs:
    counter = Counter()
    fallback_count = 0

    for log in primary_logs:
        anomaly = log.get("anomaly_type", "undefined")
        fallback = log.get("fallback_mode", False)
        counter[anomaly] += 1
        if fallback:
            fallback_count += 1

    print("\nüìä Log Summary:")
    for anomaly, count in counter.items():
        print(f"   ‚Ä¢ {anomaly:20} ‚Üí {count} logs")
    print(f"   ‚Ä¢ fallback_mode=True       ‚Üí {fallback_count} logs")
    print(f"   ‚Ä¢ malformed logs skipped   ‚Üí {malformed_count}")
